{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87f7bf9",
   "metadata": {},
   "source": [
    "# I.Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159543d",
   "metadata": {},
   "source": [
    "## I.i Setup libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2264ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, broadcast\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker_pyspark\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2e6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get region\n",
    "region = boto3.session.Session().region_name\n",
    "# get bucket: hang's project 8 of openclassrooms\n",
    "bucket_name = 'hangp8'\n",
    "img_sample_dir = 's3://{}/train_sample1/'.format(bucket_name)\n",
    "img_sample_output = 's3a://{}/output_sample/'.format(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc12a33",
   "metadata": {},
   "source": [
    "## I.ii Setup of SparkSession \n",
    "(ref: https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-spark/pyspark_mnist/pyspark_mnist_kmeans.html#Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54dbc856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-82-26.eu-west-1.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7eff7d93d828>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(jars)\n",
    "\n",
    "# Sets the Spark master URL to connect to, run locally with all cores available: 'local[*]'\n",
    "spark = (\n",
    "    SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764eb8e",
   "metadata": {},
   "source": [
    "# II. Load image data\n",
    "Access your files in S3 without copying files into your instance storage<br>\n",
    "(ref:https://sagemaker-examples.readthedocs.io/en/latest/ingest_data/013_Ingest_image_data_v1.html?highlight=fs.ls#Method-1:-Streaming-data-from-S3-to-the-SageMaker-instance-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe94bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save img path and labels\n",
    "def get_path_label_list(prefix='train_sample1/', bucket_name=bucket_name):\n",
    "    s3 = boto3.resource('s3', region_name=region)\n",
    "    bucket=bucket_name\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    path_label_list = []\n",
    "    for (bucket_name, key) in map(lambda x: (x.bucket_name, x.key), my_bucket.objects.filter(Prefix=prefix)):\n",
    "        # save img path\n",
    "        img_location = \"s3://{}/{}\".format(bucket_name, key)\n",
    "        # save img label\n",
    "        img_label = img_location.split('/')[-2]\n",
    "        path_label_list.append((img_location, img_label))\n",
    "    # remove the root folder\n",
    "    return path_label_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27970474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show 1 line of path: ('s3://hangp8/train_sample/Apple Pink Lady/0_100.jpg', 'Apple Pink Lady')\n",
      "There are 1243 images in train_sample.\n"
     ]
    }
   ],
   "source": [
    "# check result of function\n",
    "path_label_list = get_path_label_list()\n",
    "print('Show 1 line of path: {}\\nThere are {} images in train_sample.'.format(path_label_list[0], len(path_label_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d9a81dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark dataframe\n",
    "def create_df(prefix='train_sample1', bucket=bucket_name):\n",
    "    data = get_path_label_list(prefix, bucket)\n",
    "    columns = ['path', 'label']\n",
    "    df_data = spark.createDataFrame(data).toDF(*columns)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859789f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+------------------+\n",
      "|path                                                  |label             |\n",
      "+------------------------------------------------------+------------------+\n",
      "|s3://hangp8/train_sample1/Apple Golden Small/0_100.jpg|Apple Golden Small|\n",
      "|s3://hangp8/train_sample1/Apple Golden Small/1_100.jpg|Apple Golden Small|\n",
      "|s3://hangp8/train_sample1/Apple Golden Small/2_100.jpg|Apple Golden Small|\n",
      "|s3://hangp8/train_sample1/Apple Golden Small/3_100.jpg|Apple Golden Small|\n",
      "|s3://hangp8/train_sample1/Apple Golden Small/4_100.jpg|Apple Golden Small|\n",
      "|s3://hangp8/train_sample1/Apple Golden Small/5_100.jpg|Apple Golden Small|\n",
      "|s3://hangp8/train_sample1/Apple Golden Small/6_100.jpg|Apple Golden Small|\n",
      "+------------------------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a initial pyspark dataframe\n",
    "df_data = create_df()\n",
    "df_data.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c655e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print shema of Dataframe\n",
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa9f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path_img):\n",
    "    \n",
    "    '''for each image: create preprocessed image array and orb descriptors'''\n",
    "    \n",
    "    my_bucket = bucket_name\n",
    "    # get in-bucket path\n",
    "    key = os.path.relpath(path_img, 's3://'+bucket_name+'/')\n",
    "    \n",
    "    image_object = boto3.resource(\"s3\", region_name=region).Bucket(my_bucket).Object(key)\n",
    "    img = Image.open(image_object.get()[\"Body\"])\n",
    "    \n",
    "    # preprocess\n",
    "    # remove 1% extreme lightest and darkest pixels then maximize image contrast\n",
    "    tmp1 = ImageOps.autocontrast(img, cutoff=1)\n",
    "    # equalize image histogram to creat a uniform distribution of grayscale\n",
    "    tmp2 = ImageOps.equalize(tmp1)\n",
    "    img_out = tmp2\n",
    "    img_prep = np.array(img_out).flatten().tolist()\n",
    "    \n",
    "    # descriptors\n",
    "    # set max feature to retain = 50\n",
    "    orb = cv.ORB_create(nfeatures=50)\n",
    "    _, des = orb.detectAndCompute(np.array(img_out),None)\n",
    "    img_des = ([] if des is None else des.flatten().tolist())\n",
    "    \n",
    "    return (img_prep, img_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86fe5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"img_prep\", ArrayType(IntegerType()), False),\n",
    "    StructField(\"img_orb_des\", ArrayType(IntegerType()), False)])\n",
    "\n",
    "preprocess_udf = udf(preprocess, schema)\n",
    "\n",
    "df_data = df_data.withColumn(\"Output\", preprocess_udf('path'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08fa0788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- Output: struct (nullable = true)\n",
      " |    |-- img_prep: array (nullable = false)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- img_orb_des: array (nullable = false)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3401410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|                path|             label|            img_prep|         img_orb_des|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|s3://hangp8/train...|Apple Golden Small|[255, 255, 255, 2...|[191, 253, 248, 2...|\n",
      "|s3://hangp8/train...|Apple Golden Small|[255, 255, 255, 2...|[206, 176, 184, 2...|\n",
      "|s3://hangp8/train...|Apple Golden Small|[255, 255, 255, 2...|[190, 220, 126, 2...|\n",
      "|s3://hangp8/train...|Apple Golden Small|[255, 255, 255, 2...|[207, 188, 255, 2...|\n",
      "|s3://hangp8/train...|Apple Golden Small|[255, 255, 255, 2...|[253, 151, 125, 2...|\n",
      "|s3://hangp8/train...|Apple Golden Small|[255, 255, 243, 2...|[139, 29, 255, 24...|\n",
      "|s3://hangp8/train...|Apple Golden Small|[255, 255, 255, 2...|[190, 252, 110, 1...|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_data.select('path', 'label', 'Output.*').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93912fff",
   "metadata": {},
   "source": [
    "### KMEANS ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e113854",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(img_orb_des=array([[191, 253, 248, 250, 159, 245, 223, 222,  83, 127, 180,  31,  42,\n",
       "        253,  95, 173, 221, 219, 149, 147, 190, 110, 250, 179, 251, 221,\n",
       "        157, 127, 255,  93, 222, 223],\n",
       "       [217, 151, 125, 229,  36, 118,  95, 187,  49, 106, 127, 118, 223,\n",
       "         63, 154, 243, 119, 213, 181, 238, 249, 244, 121, 143, 230, 251,\n",
       "        227, 132,  47, 225, 200, 113],\n",
       "       [190, 220, 126, 250, 207, 247, 159, 254, 155,  95, 221,  31, 108,\n",
       "        255,  62, 205, 255, 251, 183, 135, 252, 199,  58, 255, 251, 255,\n",
       "        223, 219, 183,  95, 173, 223],\n",
       "       [207, 188, 255, 214, 183, 255, 189, 179, 187, 255,  23,  52, 252,\n",
       "        255, 222, 175, 167, 119, 253,  19, 245, 221, 187, 230, 107,  95,\n",
       "        127, 195, 142, 228, 205, 119],\n",
       "       [142, 189, 191, 254, 175, 246, 191, 214, 187,  95, 215, 183, 124,\n",
       "        255, 191, 175, 247, 127, 181, 147, 177, 205, 186, 235, 123, 127,\n",
       "        254, 131, 135, 245, 167, 123],\n",
       "       [138, 156, 255, 254, 247, 246, 255, 246, 153,  79, 124,  50, 252,\n",
       "        255, 175, 175, 239, 115, 183, 131, 177, 205,  58, 235,  99, 255,\n",
       "        254, 131, 142, 245, 229, 251],\n",
       "       [254, 244, 244, 242, 207, 125,  85, 155, 219, 255, 181,  63,  94,\n",
       "        253,  79, 191, 141, 215, 245,  19, 191, 118, 250, 226, 219, 219,\n",
       "         85, 255, 235, 125, 206,  87],\n",
       "       [205, 159, 124, 253, 160,  54, 127, 186,  48, 106, 118, 116, 221,\n",
       "        127, 154, 115,  23,  87, 125, 234, 241, 212, 121, 191, 102, 239,\n",
       "        225, 196, 175, 241, 201, 113],\n",
       "       [219, 118, 253, 246, 169, 118,  95,  61,  57, 202, 181, 116, 127,\n",
       "        127, 143,  59, 150, 215, 253, 170, 237, 198, 123, 202, 227, 251,\n",
       "        117, 146,  42, 245, 207, 123],\n",
       "       [191,  86, 204, 247, 172, 119, 126, 248, 151, 250, 151, 215, 219,\n",
       "        111,  59, 235, 127, 197, 255, 239, 237, 215,  91, 234, 254, 251,\n",
       "        239, 149, 191, 255, 121,  81],\n",
       "       [174, 180, 185, 250, 253, 253, 219, 223,  27,  95, 189,  29, 142,\n",
       "        253, 126, 152, 221,  27, 151,  21, 183, 111,  58, 243, 251, 221,\n",
       "        158, 255, 154,  76, 255, 247],\n",
       "       [207, 180, 185, 250, 189, 253, 121, 223,  63,  95, 181,  61, 159,\n",
       "        223, 250, 121, 223, 155, 223,  61, 187, 100, 122, 247, 239, 223,\n",
       "         62, 254, 154, 205, 213, 115],\n",
       "       [ 72, 251, 253, 245, 236,  79, 109, 191,  55, 251, 246, 104, 255,\n",
       "        119, 208, 114,  95, 214, 127, 168, 249, 250, 251, 207, 239, 255,\n",
       "        247, 140, 166, 160, 230, 121],\n",
       "       [141, 252, 253, 245, 124,  95, 255, 186, 127, 250, 246, 108, 251,\n",
       "        121, 151,  51,  95, 204, 126, 110, 225, 244, 253, 223, 238, 223,\n",
       "        231, 180, 239, 233,  72, 241],\n",
       "       [253, 180, 253, 253,  60,  63, 255, 184,  94, 250, 255, 126, 219,\n",
       "        124, 159,  97,  95, 196, 127,  47, 169, 246, 255, 159, 254, 223,\n",
       "        231, 180, 124, 235,  80, 241],\n",
       "       [255,  86, 236, 247, 172, 118,  94, 184, 159, 250, 183,  87, 219,\n",
       "        111,  95, 123,  95, 196, 255, 238, 233, 103, 219, 232, 251, 251,\n",
       "        231, 156, 174, 251, 248,  80],\n",
       "       [205, 124, 253, 245, 188, 127, 255, 186, 126, 234, 247, 124, 219,\n",
       "         61, 151, 107,  95, 254, 127,  46, 169, 244, 253, 190, 238, 223,\n",
       "        231, 180, 238, 249,  64, 241]]), img_prep=array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]]), label='Apple Golden Small', path='s3://hangp8/train_sample1/Apple Golden Small/0_100.jpg')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert flatten descriptors to orb ndarray descriptors (n, 32)\n",
    "rdd_data = df_data.rdd.map(lambda x: Row(path = x.path,\\\n",
    "                                         label = x.label,\\\n",
    "                                         img_prep = np.array(x['Output']['img_prep']).reshape(100, 100, 3),\\\n",
    "                                         img_orb_des = np.array(x['Output']['img_orb_des']).reshape(-1, 32)\n",
    "                                        )\n",
    "                          )\n",
    "# check orb descriptors of 1st image\n",
    "rdd_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d480cf80",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://hangp8/train_sample1/Apple Golden Small/0_100.jpg',\n",
       " 'Apple Golden Small',\n",
       " array([[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       " \n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]]),\n",
       " array([191, 253, 248, 250, 159, 245, 223, 222,  83, 127, 180,  31,  42,\n",
       "        253,  95, 173, 221, 219, 149, 147, 190, 110, 250, 179, 251, 221,\n",
       "        157, 127, 255,  93, 222, 223]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_desp = rdd_data.flatMap(lambda row: Row(row['path'], row['label'], row['img_prep'], i) for i in row['img_orb_des']])\n",
    "all_desp.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3aeb1b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([191, 253, 248, 250, 159, 245, 223, 222,  83, 127, 180,  31,  42,\n",
       "       253,  95, 173, 221, 219, 149, 147, 190, 110, 250, 179, 251, 221,\n",
       "       157, 127, 255,  93, 222, 223])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_desp.map(lambda row: row[3]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "515ba04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "# k is n_cluster, suppose 10*categories\n",
    "k = 10\n",
    "def create_model(rdd, k):\n",
    "    # can't select column by name, only by position\n",
    "    rdd_desp = rdd.map(lambda row: row[3])\n",
    "    model = KMeans.train(rdd_desp, k, maxIterations=10, initializationMode=\"random\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56eecc0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kmeans = create_model(all_desp, k)\n",
    "model_kmeans.predict(all_desp.take(1)[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f20bb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[191, 253, 248, 250, 159, 245, 223, 222,  83, 127, 180,  31,  42,\n",
       "        253,  95, 173, 221, 219, 149, 147, 190, 110, 250, 179, 251, 221,\n",
       "        157, 127, 255,  93, 222, 223],\n",
       "       [217, 151, 125, 229,  36, 118,  95, 187,  49, 106, 127, 118, 223,\n",
       "         63, 154, 243, 119, 213, 181, 238, 249, 244, 121, 143, 230, 251,\n",
       "        227, 132,  47, 225, 200, 113],\n",
       "       [190, 220, 126, 250, 207, 247, 159, 254, 155,  95, 221,  31, 108,\n",
       "        255,  62, 205, 255, 251, 183, 135, 252, 199,  58, 255, 251, 255,\n",
       "        223, 219, 183,  95, 173, 223],\n",
       "       [207, 188, 255, 214, 183, 255, 189, 179, 187, 255,  23,  52, 252,\n",
       "        255, 222, 175, 167, 119, 253,  19, 245, 221, 187, 230, 107,  95,\n",
       "        127, 195, 142, 228, 205, 119],\n",
       "       [142, 189, 191, 254, 175, 246, 191, 214, 187,  95, 215, 183, 124,\n",
       "        255, 191, 175, 247, 127, 181, 147, 177, 205, 186, 235, 123, 127,\n",
       "        254, 131, 135, 245, 167, 123],\n",
       "       [138, 156, 255, 254, 247, 246, 255, 246, 153,  79, 124,  50, 252,\n",
       "        255, 175, 175, 239, 115, 183, 131, 177, 205,  58, 235,  99, 255,\n",
       "        254, 131, 142, 245, 229, 251],\n",
       "       [254, 244, 244, 242, 207, 125,  85, 155, 219, 255, 181,  63,  94,\n",
       "        253,  79, 191, 141, 215, 245,  19, 191, 118, 250, 226, 219, 219,\n",
       "         85, 255, 235, 125, 206,  87],\n",
       "       [205, 159, 124, 253, 160,  54, 127, 186,  48, 106, 118, 116, 221,\n",
       "        127, 154, 115,  23,  87, 125, 234, 241, 212, 121, 191, 102, 239,\n",
       "        225, 196, 175, 241, 201, 113],\n",
       "       [219, 118, 253, 246, 169, 118,  95,  61,  57, 202, 181, 116, 127,\n",
       "        127, 143,  59, 150, 215, 253, 170, 237, 198, 123, 202, 227, 251,\n",
       "        117, 146,  42, 245, 207, 123],\n",
       "       [191,  86, 204, 247, 172, 119, 126, 248, 151, 250, 151, 215, 219,\n",
       "        111,  59, 235, 127, 197, 255, 239, 237, 215,  91, 234, 254, 251,\n",
       "        239, 149, 191, 255, 121,  81],\n",
       "       [174, 180, 185, 250, 253, 253, 219, 223,  27,  95, 189,  29, 142,\n",
       "        253, 126, 152, 221,  27, 151,  21, 183, 111,  58, 243, 251, 221,\n",
       "        158, 255, 154,  76, 255, 247],\n",
       "       [207, 180, 185, 250, 189, 253, 121, 223,  63,  95, 181,  61, 159,\n",
       "        223, 250, 121, 223, 155, 223,  61, 187, 100, 122, 247, 239, 223,\n",
       "         62, 254, 154, 205, 213, 115],\n",
       "       [ 72, 251, 253, 245, 236,  79, 109, 191,  55, 251, 246, 104, 255,\n",
       "        119, 208, 114,  95, 214, 127, 168, 249, 250, 251, 207, 239, 255,\n",
       "        247, 140, 166, 160, 230, 121],\n",
       "       [141, 252, 253, 245, 124,  95, 255, 186, 127, 250, 246, 108, 251,\n",
       "        121, 151,  51,  95, 204, 126, 110, 225, 244, 253, 223, 238, 223,\n",
       "        231, 180, 239, 233,  72, 241],\n",
       "       [253, 180, 253, 253,  60,  63, 255, 184,  94, 250, 255, 126, 219,\n",
       "        124, 159,  97,  95, 196, 127,  47, 169, 246, 255, 159, 254, 223,\n",
       "        231, 180, 124, 235,  80, 241],\n",
       "       [255,  86, 236, 247, 172, 118,  94, 184, 159, 250, 183,  87, 219,\n",
       "        111,  95, 123,  95, 196, 255, 238, 233, 103, 219, 232, 251, 251,\n",
       "        231, 156, 174, 251, 248,  80],\n",
       "       [205, 124, 253, 245, 188, 127, 255, 186, 126, 234, 247, 124, 219,\n",
       "         61, 151, 107,  95, 254, 127,  46, 169, 244, 253, 190, 238, 223,\n",
       "        231, 180, 238, 249,  64, 241]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.map(lambda row: row[0]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "04de4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "desp =  all_desp.map(lambda x: x['desp_orb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b9045bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "# k is n_cluster, suppose 10*categories\n",
    "k = 10\n",
    "model = KMeans.train(desp, k, maxIterations=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4a53ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bovw(row, model, k):\n",
    "    label = row['label']\n",
    "    path= row['path']\n",
    "    des = row['desp_orb']\n",
    "    n_des = len(des)\n",
    "    bovw = np.zeros(k)\n",
    "    for des_ in des:\n",
    "        cluster = model.predict(des_)\n",
    "        #normalize freq\n",
    "        bovw[cluster] += 1/n_des\n",
    "    return Row(path=path, label=label, bovw=bovw.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3f0d314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bovw_rdd = all_desp.map(lambda x: bovw(x, model, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "047c0733",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 129, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-157-bb4e7dda086c>\", line 1, in <lambda>\n  File \"<ipython-input-156-01694bd8520e>\", line 8, in bovw\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/clustering.py\", line 260, in predict\n    x = _convert_to_vector(x)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'numpy.int64'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-157-bb4e7dda086c>\", line 1, in <lambda>\n  File \"<ipython-input-156-01694bd8520e>\", line 8, in bovw\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/clustering.py\", line 260, in predict\n    x = _convert_to_vector(x)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'numpy.int64'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-5e3d3094b48e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# clustering for each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbovw_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \"\"\"\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 129, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-157-bb4e7dda086c>\", line 1, in <lambda>\n  File \"<ipython-input-156-01694bd8520e>\", line 8, in bovw\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/clustering.py\", line 260, in predict\n    x = _convert_to_vector(x)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'numpy.int64'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-157-bb4e7dda086c>\", line 1, in <lambda>\n  File \"<ipython-input-156-01694bd8520e>\", line 8, in bovw\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/clustering.py\", line 260, in predict\n    x = _convert_to_vector(x)\n  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'numpy.int64'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# clustering for each image\n",
    "bovw_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4c4656c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_rdd = df_data.select('path', 'label').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "305d6cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(path='s3://hangp8/train_sample1/Apple Golden Small/0_100.jpg', label='Apple Golden Small'),\n",
       " Row(path='s3://hangp8/train_sample1/Apple Golden Small/1_100.jpg', label='Apple Golden Small'),\n",
       " Row(path='s3://hangp8/train_sample1/Apple Golden Small/2_100.jpg', label='Apple Golden Small'),\n",
       " Row(path='s3://hangp8/train_sample1/Apple Golden Small/3_100.jpg', label='Apple Golden Small'),\n",
       " Row(path='s3://hangp8/train_sample1/Apple Golden Small/4_100.jpg', label='Apple Golden Small'),\n",
       " Row(path='s3://hangp8/train_sample1/Apple Golden Small/5_100.jpg', label='Apple Golden Small'),\n",
       " Row(path='s3://hangp8/train_sample1/Apple Golden Small/6_100.jpg', label='Apple Golden Small'),\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.058823529411764705,\n",
       "  0.058823529411764705,\n",
       "  0.058823529411764705,\n",
       "  0.11764705882352941,\n",
       "  0.0,\n",
       "  0.29411764705882354,\n",
       "  0.23529411764705882,\n",
       "  0.1764705882352941],\n",
       " [0.1, 0.0, 0.1, 0.0, 0.05, 0.1, 0.0, 0.2, 0.25, 0.2],\n",
       " [0.047619047619047616,\n",
       "  0.047619047619047616,\n",
       "  0.09523809523809523,\n",
       "  0.0,\n",
       "  0.047619047619047616,\n",
       "  0.14285714285714285,\n",
       "  0.0,\n",
       "  0.19047619047619047,\n",
       "  0.3333333333333333,\n",
       "  0.09523809523809523],\n",
       " [0.15000000000000002,\n",
       "  0.05,\n",
       "  0.15000000000000002,\n",
       "  0.0,\n",
       "  0.05,\n",
       "  0.15000000000000002,\n",
       "  0.0,\n",
       "  0.25,\n",
       "  0.15000000000000002,\n",
       "  0.05],\n",
       " [0.09090909090909091,\n",
       "  0.045454545454545456,\n",
       "  0.045454545454545456,\n",
       "  0.045454545454545456,\n",
       "  0.0,\n",
       "  0.18181818181818182,\n",
       "  0.0,\n",
       "  0.18181818181818182,\n",
       "  0.2272727272727273,\n",
       "  0.18181818181818182],\n",
       " [0.2, 0.05, 0.1, 0.05, 0.05, 0.15000000000000002, 0.05, 0.1, 0.05, 0.2],\n",
       " [0.09523809523809523,\n",
       "  0.047619047619047616,\n",
       "  0.09523809523809523,\n",
       "  0.047619047619047616,\n",
       "  0.0,\n",
       "  0.23809523809523808,\n",
       "  0.0,\n",
       "  0.09523809523809523,\n",
       "  0.047619047619047616,\n",
       "  0.3333333333333333]]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_rdd = df_data_rdd.union(bovw_rdd)\n",
    "df_final_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f1b2729e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(path='s3://hangp8/train_sample1/Apple Golden Small/0_100.jpg', label='Apple Golden Small')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ORB(img):\n",
    "    # read image in pyspark dataframe, which are saved as flatten list\n",
    "    arr = np.array(img).reshape(100, 100, 3)\n",
    "    \n",
    "    arr = preprocess_input(arr)\n",
    "    arr = arr.flatten().tolist()\n",
    "    feature = model.predict(arr.reshape(-1, 3, 100, 100))\n",
    "    feature = feature.flatten().tolist()\n",
    "    return feature\n",
    "\n",
    "img_pred_UDF = udf(lambda img: img_pred(img))\n",
    "df_data.withColumn('NameCol', img_pred_UDF('Output.img_prep')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f7246be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import deserialize, serialize\n",
    "from tensorflow.python.keras.saving import saving_utils\n",
    "\n",
    "\n",
    "def unpack(model, training_config, weights):\n",
    "    restored_model = deserialize(model)\n",
    "    if training_config is not None:\n",
    "        restored_model.compile(\n",
    "            **saving_utils.compile_args_from_training_config(\n",
    "                training_config\n",
    "            )\n",
    "        )\n",
    "    restored_model.set_weights(weights)\n",
    "    return restored_model\n",
    "\n",
    "# Hotfix function\n",
    "def make_keras_picklable():\n",
    "\n",
    "    def __reduce__(self):\n",
    "        model_metadata = saving_utils.model_metadata(self)\n",
    "        training_config = model_metadata.get(\"training_config\", None)\n",
    "        model = serialize(self)\n",
    "        weights = self.get_weights()\n",
    "        return (unpack, (model, training_config, weights))\n",
    "\n",
    "    cls = Model\n",
    "    cls.__reduce__ = __reduce__\n",
    "\n",
    "# Run the function\n",
    "make_keras_picklable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ece98181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# add ResNet50 pre-trained by ImageNet and remove fully-connected layers\n",
    "model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(3, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34c12199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_new = sc.broadcast(model)\n",
    "def img_pred(img):\n",
    "    # read image in pyspark dataframe, which are saved as flatten list\n",
    "    arr = np.array(img).reshape(100, 100, 3)\n",
    "    \n",
    "    arr = preprocess_input(arr)\n",
    "    arr = arr.flatten().tolist()\n",
    "    feature = model.predict(arr.reshape(-1, 3, 100, 100))\n",
    "    feature = feature.flatten().tolist()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c7f7124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|                path|             label|              Output|             NameCol|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "|s3://hangp8/train...|Apple Golden Small|[[255, 255, 255, ...|[151.061004638671...|\n",
      "+--------------------+------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img_pred_UDF = udf(lambda img: img_pred(img))\n",
    "df_data.withColumn('NameCol', img_pred_UDF('Output.img_prep')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61c434a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "class PickableSequential(Sequential):\n",
    "    def __getstate__(self):\n",
    "        state = super().__getstate__()\n",
    "        state.pop(\"_trackable_saver\")\n",
    "        state.pop(\"_compiled_trainable_state\")\n",
    "        return state\n",
    "\n",
    "model_pickable = PickableSequential(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8cb6210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/serializers.py\", line 587, in dumps\n",
      "    return cloudpickle.dumps(obj, 2)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\", line 863, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\", line 260, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 409, in dump\n",
      "    self.save(obj)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 736, in save_tuple\n",
      "    save(element)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\", line 400, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 852, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\", line 400, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 521, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 634, in save_reduce\n",
      "    save(state)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 521, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 634, in save_reduce\n",
      "    save(state)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 851, in _batch_setitems\n",
      "    save(k)\n",
      "  File \"/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\", line 496, in save\n",
      "    rv = reduce(self.proto)\n",
      "TypeError: can't pickle weakref objects\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: can't pickle weakref objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m             \u001b[0;31m# Subtle.  Same as in the big comment below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 or themodule is None):\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qualname'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    851\u001b[0m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 or themodule is None):\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qualname'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    850\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle weakref objects",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-45b3593d1df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_pred_UDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NameCol'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_pred_UDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output.img_prep'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# and should have a minimal performance impact.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_judf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_create_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mjdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseDataType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, returnType)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m     37\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: TypeError: can't pickle weakref objects"
     ]
    }
   ],
   "source": [
    "img_pred_UDF = udf(lambda img: img_pred(img))\n",
    "df_data.withColumn('NameCol', img_pred_UDF('Output.img_prep')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e5f1e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "  \"\"\"\n",
    "  Returns a vgg16 model with top layer removed and broadcasted pretrained weights.\n",
    "  \"\"\"\n",
    "  model = VGG16(weights=None, include_top=False, input_shape=(100, 100, 3))\n",
    "  model.set_weights(bc_model_weights.value)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a658330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc6383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c67678e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import boto3\n",
    "\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "client = boto3.client('s3')\n",
    "client.upload_file(Filename='my_model.h5',\n",
    "                  Bucket=bucket_name,\n",
    "                  Key='my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3f6ec503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "client.download_file(bucket_name,\n",
    "                     'my_model.h5',\n",
    "                     'my_model.h5')\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "84110ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toarray(img_prep):\n",
    "    arr = np.array(img_prep).reshape(100, 100, 3)\n",
    "    arr = preprocess_input(arr)\n",
    "    return arr\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"Feturize a pd.Series of raw images using the input model.\n",
    "    return: a pd.Series of image features\"\"\"\n",
    "  \n",
    "    imgs = np.stack(content_series.map(toarray))\n",
    "    preds = model.predict(imgs)\n",
    "    # the output would be 3*3*512\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ec49bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.0.0\n",
      "  Downloading pyspark-3.0.0.tar.gz (204.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 204.7 MB 17 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9 in /home/ec2-user/anaconda3/envs/mxnet_latest_p37/lib/python3.7/site-packages (from pyspark==3.0.0) (0.10.9)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044182 sha256=733a71ae07657fcb6e10148129091eb4cfacaa2f65c4a31494e6afdf7e1da5ea\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4e/c5/36/aef1bb711963a619063119cc032176106827a129c0be20e301\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.1.2\n",
      "    Uninstalling pyspark-3.1.2:\n",
      "      Successfully uninstalled pyspark-3.1.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-pyspark 1.4.2 requires pyspark==2.4.0, but you have pyspark 3.0.0 which is incompatible.\u001b[0m\n",
      "Successfully installed pyspark-3.0.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_latest_p37/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d6db36f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'PandasUDFType' has no attribute 'SCALARITER'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-87cdc3677783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mpandas_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'array<float>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPandasUDFType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCALARITER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeaturize_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_series_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n\u001b[1;32m      5\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mdecorator\u001b[0m \u001b[0mspecifies\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mSpark\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mArrayType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloatType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'PandasUDFType' has no attribute 'SCALARITER'"
     ]
    }
   ],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALARITER)\n",
    "\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    content_series_iter: This argument is an iterator over batches of data, \n",
    "    where each batch is a pandas Series of image data.'''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    #feature = pd.Series()\n",
    "    #for content_series in content_series_iter:\n",
    "        #feature.append(featurize_series(model, content_series))\n",
    "    return featurize_series(model, content_series_iter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d0d6068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas UDFs on large records (e.g., very large images) can run into Out Of Memory (OOM) errors.\n",
    "# If you hit such errors in the cell below, try reducing the Arrow batch size via `maxRecordsPerBatch`.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "91b7db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now run featurization on our entire Spark DataFrame.\n",
    "# NOTE: This can take a long time (about 10 minutes) since it applies a large model to the full dataset.\n",
    "features_df = df_tt10.select(col(\"img_prep\"), featurize_udf(\"img_prep\").alias(\"cnn_features\"))\n",
    "#features_df.write.mode(\"overwrite\").parquet(\"dbfs:/ml/tmp/flower_photos_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "71363d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- img_prep: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- cnn_features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "042d5ecc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2496.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 90.0 failed 1 times, most recent failure: Lost task 0.0 in stage 90.0 (TID 105, localhost, executor driver): java.lang.IllegalArgumentException\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-271-16a5f9fe7e2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# Sets an explain mode depending on a given argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_no_argument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mexplain_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"simple\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_extended_case\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mexplain_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"extended\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mextended\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"simple\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCapturedException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mFailed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mexecute\u001b[0m \u001b[0ma\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \"\"\"\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2496.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 90.0 failed 1 times, most recent failure: Lost task 0.0 in stage 90.0 (TID 105, localhost, executor driver): java.lang.IllegalArgumentException\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor99.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a5ef4c2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1200.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 40, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1377, in verify_struct\n    % (obj, type(obj))))\nTypeError: StructType can not accept object 4596736 in type <class 'int'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1377, in verify_struct\n    % (obj, type(obj))))\nTypeError: StructType can not accept object 4596736 in type <class 'int'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-d30a289418f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArrayType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tt10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1200.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 40, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1377, in verify_struct\n    % (obj, type(obj))))\nTypeError: StructType can not accept object 4596736 in type <class 'int'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/session.py\", line 730, in prepare\n    verify_func(obj)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1389, in verify\n    verify_value(obj)\n  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\", line 1377, in verify_struct\n    % (obj, type(obj))))\nTypeError: StructType can not accept object 4596736 in type <class 'int'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField(\"sum\", ArrayType(IntegerType()), False)])\n",
    "df_test = df_tt10.rdd.map(lambda x: x)\n",
    "df_test.toDF(schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c70129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# content of img_prep: 3*100pixel*100pixel\n",
    "len(df_data.select('Output.img_prep').take(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "063f35bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# content of img_orb_des: descriptors detected: 32(orb descriptor lenth) * n(number of descriptors)\n",
    "len(df_data.select('Output.img_orb_des').take(2)[0][0]), len(df_data.select('Output.img_orb_des').take(2)[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14c32dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+\n",
      "|                path|          label|            img_prep|         img_orb_des|\n",
      "+--------------------+---------------+--------------------+--------------------+\n",
      "|s3://hangp8/train...|Apple Pink Lady|[255, 255, 255, 2...|[252, 252, 61, 12...|\n",
      "|s3://hangp8/train...|Apple Pink Lady|[255, 255, 255, 2...|[223, 22, 255, 24...|\n",
      "|s3://hangp8/train...|Apple Pink Lady|[255, 255, 255, 2...|[237, 253, 189, 1...|\n",
      "|s3://hangp8/train...|Apple Pink Lady|[255, 255, 255, 2...|[223, 22, 255, 25...|\n",
      "|s3://hangp8/train...|Apple Pink Lady|[255, 255, 255, 2...|[191, 180, 253, 2...|\n",
      "+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# img_prep : list of 30000 elements\n",
    "# img_orb_des : list of 32*n elements, where n = 0,1,2,3...\n",
    "df_data.select('path', 'label', 'Output.*').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f033c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sparkdl\n",
      "  Downloading sparkdl-0.2.2-py3-none-any.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sparkdl\n",
      "Successfully installed sparkdl-0.2.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sparkdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a61e319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35bc8029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorframes\n",
      "  Downloading tensorframes-0.2.9-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: tensorframes\n",
      "Successfully installed tensorframes-0.2.9\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae20c09d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka\n",
      "  Downloading kafka-1.3.5-py2.py3-none-any.whl (207 kB)\n",
      "\u001b[K     |████████████████████████████████| 207 kB 56.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: kafka\n",
      "Successfully installed kafka-1.3.5\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec17f466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowonspark\n",
      "  Downloading tensorflowonspark-2.2.4-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 4.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>38.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflowonspark) (49.6.0.post20210108)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflowonspark) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->tensorflowonspark) (2.4.7)\n",
      "Installing collected packages: tensorflowonspark\n",
      "Successfully installed tensorflowonspark-2.2.4\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflowonspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07ae3a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from keras) (1.5.3)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from h5py->keras) (1.5.1)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e03665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp36-cp36m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 454.3 MB 12 kB/s s eta 0:00:01     |███████████████████████████████▍| 446.0 MB 60.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp36-cp36m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 19.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 49.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 38.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 582 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 73.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 81.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow) (1.5.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 60.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.34.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 88.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (49.6.0.post20210108)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.5-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 76.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 74.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 85.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=ce78dd08edcdaa5e2ad87f41e598f0fd5e38076e09667593af14cd4d0a27b464\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-nightly, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.34.0 google-auth-oauthlib-0.4.5 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0097e558",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=cd9b219f797d194fbbb9f6be7895f2043ef05b579b62ad0c793660e7d68b634c\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/17/a7/8b/a7e03881534e78558920ac68aaeca05180c0e2c3d11c4fce3b\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31a042fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sparkdl' has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-df1b4ebdeb3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepImageFeaturizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# extracting feature from images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m featurizer = DeepImageFeaturizer(inputCol=\"image\",\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sparkdl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimageIO\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimageSchema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimageType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasImageFileTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepImagePredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeepImageFeaturizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFImageTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sparkdl/transformers/keras_image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeConverters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKSessionWrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m from sparkdl.param import (\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sparkdl' has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "#import sparkdl\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from sparkdl import DeepImageFeaturizer \n",
    "# extracting feature from images\n",
    "featurizer = DeepImageFeaturizer(inputCol=\"image\",\n",
    "                                 outputCol=\"features\",\n",
    "                                 modelName=\"InceptionV3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e731e5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sparkdl' has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b695fa030dad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sparkdl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimageIO\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimageSchema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimageType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadImages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasImageFileTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepImagePredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeepImageFeaturizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFImageTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/sparkdl/transformers/keras_image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeConverters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msparkdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKSessionWrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m from sparkdl.param import (\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sparkdl' has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "from sparkdl import KerasTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44402409",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2a2f46b9a649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output.img_prep'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sparkdl/transformers/named_image.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    156\u001b[0m                                              \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                                              modelName=self.getModelName(), featurize=True)\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sparkdl/transformers/named_image.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mmodelGraphSpec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_buildTFGraphForName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetModelName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetFeaturize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0minputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mresizedCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"__sdl_imagesResized\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sparkdl/transformers/named_image.py\u001b[0m in \u001b[0;36m_buildTFGraphForName\u001b[0;34m(name, featurize)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mCurrently\u001b[0m \u001b[0monly\u001b[0m \u001b[0msupports\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrained\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mKeras\u001b[0m \u001b[0mapplications\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \"\"\"\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mmodelData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras_apps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetKerasApplicationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetModelData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"session\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0moutputTensorName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputTensorName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sparkdl/transformers/keras_applications.py\u001b[0m in \u001b[0;36mgetModelData\u001b[0;34m(self, featurize)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetModelData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "featurizer.transform(df_data.select('Output.img_prep').take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1aea2dcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e477f785c23d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# add ResNet50 pre-trained by ImageNet and remove fully-connected layers\n",
    "model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(3, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f8499d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 3, 512)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_ = preprocess_input(img) # preprocess image as VGG16 model want (normalize pixel value to (-1,1))\n",
    "model.predict(img_.reshape(-1, 100, 100, 3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36dfc2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cnn = model.predict(img_.reshape(-1, 100, 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ebf07f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 10.22053050994873,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.8791835308074951,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 18.117979049682617,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.977135181427002,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11366993188858032,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.24408940970897675,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 9.639677047729492,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 19.597660064697266,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.611595630645752,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 14.506049156188965,\n",
       " 34.555381774902344,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cnn.flatten().tolist()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "377f26ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6f62300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1243"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.select('Output.img_prep').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ebc6e19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'pyspark.sql.dataframe.DataFrame'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c9d0b0199db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output.img_prep'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1704\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m     self._adapter = adapter_cls(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 994\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    995\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'pyspark.sql.dataframe.DataFrame'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "model.predict(df_data.select('Output.img_prep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4e414",
   "metadata": {},
   "source": [
    "# Export data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1da39750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.write.parquet('s3a://{}/output_sample/'.format(bucket_name), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38b0be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = spark.read.parquet('s3a://hangp8/output_sample/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bb2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "156b2117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- Output: struct (nullable = true)\n",
      " |    |-- img_prep: array (nullable = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- img_orb_des: array (nullable = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b55eaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------------------+\n",
      "|                path|          label|            img_prep|         img_orb_des|\n",
      "+--------------------+---------------+--------------------+--------------------+\n",
      "|s3://hangp8/train...|Apple Pink Lady|[255, 255, 255, 2...|[252, 252, 61, 12...|\n",
      "|s3://hangp8/train...|Apple Pink Lady|[255, 255, 255, 2...|[223, 22, 255, 24...|\n",
      "|s3://hangp8/train...|Apple Pink Lady|[255, 255, 255, 2...|[237, 253, 189, 1...|\n",
      "|s3://hangp8/train...|Apple Pink Lady|[255, 255, 255, 2...|[223, 22, 255, 25...|\n",
      "|s3://hangp8/train...|Apple Pink Lady|[255, 255, 255, 2...|[191, 180, 253, 2...|\n",
      "+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check.select('path', 'label', 'Output.*').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67525afc",
   "metadata": {},
   "source": [
    "# Brouillons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30da4ce",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Import of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d91ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>client_installments_AMT_PAYMENT_min_sum</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>bureau_DAYS_CREDIT_ENDDATE_max</th>\n",
       "      <th>...</th>\n",
       "      <th>previous_PRODUCT_COMBINATION_Cash Street: high_sum</th>\n",
       "      <th>client_credit_AMT_DRAWINGS_CURRENT_max_min</th>\n",
       "      <th>NAME_FAMILY_STATUS_Civil marriage</th>\n",
       "      <th>client_cash_NAME_CONTRACT_STATUS_Returned to the store_mean_max</th>\n",
       "      <th>client_installments_NUM_INSTALMENT_NUMBER_min_max</th>\n",
       "      <th>previous_CHANNEL_TYPE_Stone_mean</th>\n",
       "      <th>previous_WEEKDAY_APPR_PROCESS_START_THURSDAY_sum</th>\n",
       "      <th>client_bureau_balance_STATUS_C_count_min</th>\n",
       "      <th>client_credit_CNT_DRAWINGS_POS_CURRENT_sum_max</th>\n",
       "      <th>OCCUPATION_TYPE_High skill tech staff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.752614</td>\n",
       "      <td>0.789654</td>\n",
       "      <td>0.159520</td>\n",
       "      <td>27746.775</td>\n",
       "      <td>-19241</td>\n",
       "      <td>568800.0</td>\n",
       "      <td>20560.5</td>\n",
       "      <td>-2329</td>\n",
       "      <td>1778.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.564990</td>\n",
       "      <td>0.291656</td>\n",
       "      <td>0.432962</td>\n",
       "      <td>43318.800</td>\n",
       "      <td>-18064</td>\n",
       "      <td>222768.0</td>\n",
       "      <td>17370.0</td>\n",
       "      <td>-4469</td>\n",
       "      <td>1324.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.699787</td>\n",
       "      <td>0.610991</td>\n",
       "      <td>66875.266</td>\n",
       "      <td>-20038</td>\n",
       "      <td>663264.0</td>\n",
       "      <td>69777.0</td>\n",
       "      <td>-4458</td>\n",
       "      <td>-567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157500.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.525734</td>\n",
       "      <td>0.509677</td>\n",
       "      <td>0.612704</td>\n",
       "      <td>172044.310</td>\n",
       "      <td>-13976</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>49018.5</td>\n",
       "      <td>-1866</td>\n",
       "      <td>30885.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22823.550</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.202145</td>\n",
       "      <td>0.425687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133169.400</td>\n",
       "      <td>-13040</td>\n",
       "      <td>625500.0</td>\n",
       "      <td>32067.0</td>\n",
       "      <td>-2191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.628904</td>\n",
       "      <td>0.392774</td>\n",
       "      <td>1842460.900</td>\n",
       "      <td>-18604</td>\n",
       "      <td>959688.0</td>\n",
       "      <td>34600.5</td>\n",
       "      <td>-12009</td>\n",
       "      <td>8957.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87750.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.760851</td>\n",
       "      <td>0.571084</td>\n",
       "      <td>0.651260</td>\n",
       "      <td>68259.330</td>\n",
       "      <td>-16685</td>\n",
       "      <td>499221.0</td>\n",
       "      <td>22117.5</td>\n",
       "      <td>-2580</td>\n",
       "      <td>5813.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.565290</td>\n",
       "      <td>0.613033</td>\n",
       "      <td>0.312365</td>\n",
       "      <td>90619.650</td>\n",
       "      <td>-9516</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>14220.0</td>\n",
       "      <td>-1387</td>\n",
       "      <td>27225.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.718507</td>\n",
       "      <td>0.808788</td>\n",
       "      <td>0.522697</td>\n",
       "      <td>451534.880</td>\n",
       "      <td>-12744</td>\n",
       "      <td>364896.0</td>\n",
       "      <td>28957.5</td>\n",
       "      <td>-1013</td>\n",
       "      <td>1363.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.210562</td>\n",
       "      <td>0.444848</td>\n",
       "      <td>0.194068</td>\n",
       "      <td>272273.840</td>\n",
       "      <td>-10395</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>5337.0</td>\n",
       "      <td>-2625</td>\n",
       "      <td>30886.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46897.695</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 344 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  \\\n",
       "0           0      0.752614      0.789654      0.159520   \n",
       "1           1      0.564990      0.291656      0.432962   \n",
       "2           2           NaN      0.699787      0.610991   \n",
       "3           3      0.525734      0.509677      0.612704   \n",
       "4           4      0.202145      0.425687           NaN   \n",
       "5           5           NaN      0.628904      0.392774   \n",
       "6           6      0.760851      0.571084      0.651260   \n",
       "7           7      0.565290      0.613033      0.312365   \n",
       "8           8      0.718507      0.808788      0.522697   \n",
       "9           9      0.210562      0.444848      0.194068   \n",
       "\n",
       "   client_installments_AMT_PAYMENT_min_sum  DAYS_BIRTH  AMT_CREDIT  \\\n",
       "0                                27746.775      -19241    568800.0   \n",
       "1                                43318.800      -18064    222768.0   \n",
       "2                                66875.266      -20038    663264.0   \n",
       "3                               172044.310      -13976   1575000.0   \n",
       "4                               133169.400      -13040    625500.0   \n",
       "5                              1842460.900      -18604    959688.0   \n",
       "6                                68259.330      -16685    499221.0   \n",
       "7                                90619.650       -9516    180000.0   \n",
       "8                               451534.880      -12744    364896.0   \n",
       "9                               272273.840      -10395     45000.0   \n",
       "\n",
       "   AMT_ANNUITY  DAYS_EMPLOYED  bureau_DAYS_CREDIT_ENDDATE_max  ...  \\\n",
       "0      20560.5          -2329                          1778.0  ...   \n",
       "1      17370.0          -4469                          1324.0  ...   \n",
       "2      69777.0          -4458                          -567.0  ...   \n",
       "3      49018.5          -1866                         30885.0  ...   \n",
       "4      32067.0          -2191                             NaN  ...   \n",
       "5      34600.5         -12009                          8957.0  ...   \n",
       "6      22117.5          -2580                          5813.0  ...   \n",
       "7      14220.0          -1387                         27225.0  ...   \n",
       "8      28957.5          -1013                          1363.0  ...   \n",
       "9       5337.0          -2625                         30886.0  ...   \n",
       "\n",
       "   previous_PRODUCT_COMBINATION_Cash Street: high_sum  \\\n",
       "0                                                0.0    \n",
       "1                                                0.0    \n",
       "2                                                0.0    \n",
       "3                                                0.0    \n",
       "4                                                0.0    \n",
       "5                                                1.0    \n",
       "6                                                0.0    \n",
       "7                                                0.0    \n",
       "8                                                0.0    \n",
       "9                                                1.0    \n",
       "\n",
       "   client_credit_AMT_DRAWINGS_CURRENT_max_min  \\\n",
       "0                                         NaN   \n",
       "1                                         NaN   \n",
       "2                                  157500.000   \n",
       "3                                   22823.550   \n",
       "4                                         NaN   \n",
       "5                                   87750.000   \n",
       "6                                         NaN   \n",
       "7                                         NaN   \n",
       "8                                       0.000   \n",
       "9                                   46897.695   \n",
       "\n",
       "   NAME_FAMILY_STATUS_Civil marriage  \\\n",
       "0                                  0   \n",
       "1                                  0   \n",
       "2                                  0   \n",
       "3                                  0   \n",
       "4                                  0   \n",
       "5                                  0   \n",
       "6                                  0   \n",
       "7                                  0   \n",
       "8                                  0   \n",
       "9                                  1   \n",
       "\n",
       "   client_cash_NAME_CONTRACT_STATUS_Returned to the store_mean_max  \\\n",
       "0                                                0.0                 \n",
       "1                                                0.0                 \n",
       "2                                                0.0                 \n",
       "3                                                0.0                 \n",
       "4                                                0.0                 \n",
       "5                                                0.0                 \n",
       "6                                                0.0                 \n",
       "7                                                0.0                 \n",
       "8                                                0.0                 \n",
       "9                                                0.0                 \n",
       "\n",
       "   client_installments_NUM_INSTALMENT_NUMBER_min_max  \\\n",
       "0                                                2.0   \n",
       "1                                                1.0   \n",
       "2                                                1.0   \n",
       "3                                                1.0   \n",
       "4                                                1.0   \n",
       "5                                                1.0   \n",
       "6                                                1.0   \n",
       "7                                                1.0   \n",
       "8                                                1.0   \n",
       "9                                                1.0   \n",
       "\n",
       "   previous_CHANNEL_TYPE_Stone_mean  \\\n",
       "0                              0.00   \n",
       "1                              0.00   \n",
       "2                              0.25   \n",
       "3                              0.00   \n",
       "4                              0.00   \n",
       "5                              0.00   \n",
       "6                              0.00   \n",
       "7                              0.50   \n",
       "8                              0.00   \n",
       "9                              0.00   \n",
       "\n",
       "   previous_WEEKDAY_APPR_PROCESS_START_THURSDAY_sum  \\\n",
       "0                                               0.0   \n",
       "1                                               1.0   \n",
       "2                                               1.0   \n",
       "3                                               4.0   \n",
       "4                                               0.0   \n",
       "5                                               2.0   \n",
       "6                                               0.0   \n",
       "7                                               0.0   \n",
       "8                                               0.0   \n",
       "9                                               7.0   \n",
       "\n",
       "   client_bureau_balance_STATUS_C_count_min  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       NaN   \n",
       "5                                       0.0   \n",
       "6                                       0.0   \n",
       "7                                       0.0   \n",
       "8                                       0.0   \n",
       "9                                       0.0   \n",
       "\n",
       "   client_credit_CNT_DRAWINGS_POS_CURRENT_sum_max  \\\n",
       "0                                             NaN   \n",
       "1                                             NaN   \n",
       "2                                             0.0   \n",
       "3                                           115.0   \n",
       "4                                             NaN   \n",
       "5                                             8.0   \n",
       "6                                             NaN   \n",
       "7                                             NaN   \n",
       "8                                             0.0   \n",
       "9                                            51.0   \n",
       "\n",
       "   OCCUPATION_TYPE_High skill tech staff  \n",
       "0                                      0  \n",
       "1                                      0  \n",
       "2                                      0  \n",
       "3                                      0  \n",
       "4                                      0  \n",
       "5                                      0  \n",
       "6                                      1  \n",
       "7                                      0  \n",
       "8                                      0  \n",
       "9                                      0  \n",
       "\n",
       "[10 rows x 344 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_data = \"s3://hangtestbucket/test.csv\".format(region)\n",
    "df = pd.read_csv(input_data, nrows=10)\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca610d",
   "metadata": {},
   "source": [
    "## Import and export image to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4b240cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "461f747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "def read_image_from_s3(path, region_name=region, bucket='hangtestbucket'):\n",
    "    \"\"\"Load image file from s3.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket: string\n",
    "        Bucket name\n",
    "    path : string\n",
    "        Path in s3\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np array\n",
    "        Image array\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3', region_name=region_name)\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    object = bucket.Object(path)\n",
    "    response = object.get()\n",
    "    file_stream = response['Body']\n",
    "    im = Image.open(file_stream)\n",
    "    return np.array(im)\n",
    "\n",
    "def write_image_to_s3(img_array, path, region_name=region, bucket='hangtestbucket'):\n",
    "    \"\"\"Write an image array into S3 bucket\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket: string\n",
    "        Bucket name\n",
    "    path : string\n",
    "        Path in s3\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3', region_name)\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    object = bucket.Object(path)\n",
    "    file_stream = BytesIO()\n",
    "    im = Image.fromarray(img_array)\n",
    "    im.save(file_stream, format='jpeg')\n",
    "    object.put(Body=file_stream.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95b0ad53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdf7c0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[42,42,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c12478de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_preptmpq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f79f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp11 = ImageOps.autocontrast(img, cutoff=1)\n",
    "# equalize image histogram to creat a uniform distribution of grayscale\n",
    "#tmp22 = ImageOps.equalize(tmp1)\n",
    "\n",
    "#img_preptmp = np.array(tmp22)\n",
    "img_preptmpq = img.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a3fba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_p = img_preptmpq.reshape(100, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e8891bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7b1c01a710>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img = read_image_from_s3(path='75_100.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "213f213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_image_to_s3(img, path='export_test.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b83951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6ff26b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|              origin|width|height|\n",
      "+--------------------+-----+------+\n",
      "|s3a://hangtestbuc...|  100|   100|\n",
      "+--------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"image\").option(\"dropInvalid\", True).load(\"s3a://hangtestbucket/75_100.jpg\")\n",
    "df.select(\"image.origin\", \"image.width\", \"image.height\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
